{"/aboutme/":{"data":{"":"","who-am-i#Who am I":"Well, although it is kind of silly to define oneself, I am an INFJ, a Master’s student who want to keep exploring AI as a researcher, and a gamer. But most importantly, to answer “who am I” , is up to you:\nFor potential employers, recruiters, and professors I am looking for a PhD program in the next three years. And I am also open to internships in AI research. If you are interested in my research, please contact me by email: doby-xu@outlook.com and I will be happy to send my latest CV.\nMy advisors tend to describe me as independent-minded, creative, suck at writing, but I prefer to describe myself as a person who keeps a tight schedule and is always curious about the world.\nCheck out my work on the permutation equivariance of Transformers and its applications. I mathematically formulated, extended and proved the permutation equivariance of Transformers in both row-wise and column-wise, in both forward and backward passes.\nMy Work Accepted by CVPR'24Permutation Equivariance of Transformer For my peers and fellow researchers I am a shy boy but I am always open to collaborations and discussions. No matter what you are interested in, like how to verify the permutation equivariance of Transformers, or how to beat Malenia, I am always happy to discuss with you.\nAnd also, my blogs would also be potentially helpful to you if you are trying to read my papers.\nFor junior students Although I am not one of the top graduate students in my cohort, I still got some experience to share. My experience may be highly specialized in how to apply for a Master’s program in China, and how to survive an academic journey to the west alone, so switch to my Chinese page if you are interested in that (the switch is on the bottom left corner)."},"title":"About Me"},"/blog/":{"data":{"":"Here are some of my thoughts, notes, experiences, and stories that I have to share."},"title":"Blog"},"/research/":{"data":{"":"Here are all my research works.\nAs a boy who spent the childhood playing games, it is hard for me to resist the temptation of computer science. And as a person who studied computer science, it is hard for me to resist the temptation of AI.\nAI is growing wildly. Just when we all thought more parameters and data would not break the bottleneck, large model showed the emergence of intelligence. I am more than honored to be a part of this era, and to witness all the astonishing breakthroughs, right in the field.\nPrivacy-Preserving Split Learning via Patch Shuffling over TransformersDixi Yao\rPermutation Equivariance of Transformers and Its ApplicationsHengyuan Xu\rHufuHengyuan Xu"},"title":"Research"},"/research/cvpr24/":{"data":{"":"","backward-propagation#Backward Propagation":"Actually, check out the 5 minutes video of our work on YouTube would be a good choice! All the gist of our work is there. And here, is mostly my personal view during the research process.\n❔ Transformer is somehow invariant to token permutation, but mathematically, how? How about intra-token permutation? What happens in the backward propagation? Here’s the answer! Inspiration We all know that Transformer is somehow invariant to token permutation. Like in the famous work, Intriguing Properties of Vision Transformers, as shown in the figure below:\nthey found that ViT is quite robust to patch shuffling. We all know some of the reasons, like:\nTokens are symmetric to self-attention. Transformer can only sense position information through positional encoding. etc. etc… But all of these explanations are quite intuitive, not rigorous. My advisor, Liyao Xiang, asked me, “Why? Why is Transformer invariant to token permutation? There must be a mathematical explanation behind it.” And that’s the beginning of our research, and my transformation.\nInvariance? Equivariance! At the beginning of this project, we were also quite curious about the permutation properties, especially from a mathematical perspective. We called it “invariance” at that time. However, if we looked into it a little bit deeper, we would soon find the real property was not called invariance, but equivariance.\n$$ f(Px) = Pf(x) \\tag{equivariance} $$\n$$ f(Px) = f(x) \\tag{invariance} $$\nI, found that, Transformer rigorously satisfies Eq. (equivariance). Actually, I was definitely not the first one to find this. Eq. (equivariance) is kind of a common sense, which many researchers have no idea about. So the Transformer is literally equivariant to token permutation, like this: Wait, how does it even work? Transformer is by all means not linear. How can it satisfy Eq. (equivariance)? Well, let’s dive into Transformer. The main parts of Transformer are:\nSelf-Attention linear projection softmax Feed-Forward linear projection element-wise activation Shortcut matrix addition Except for linear projection, all the other parts are element-wise operations. Element-wise operations are permutation equivariant:\n$$ (P_1 A P_2) \\odot ( P_1 B P_2) = P_1 (A \\odot B) P_2 $$\nWhy? Because they are element-wise! On the left hand-side of the equation, $a_{ij}$ in $A$ and $b_{ij}$ in $B$ are permuted to the same position before being performed the operation. On the right hand-side, $a_{ij}$ and $b_{ij}$ are performed the operation first, then permuted to the same position. So the result is the same.\nHow about the linear projection? Well, it is linear!\n$$ P_1 (AW)= (P_1 A)W $$\nBefore we proceed, take a little quiz: are you familiar with the $P$ in Eq. (equivariance)? What does it mean? It is basic algebra, so basic that many people would forget about it. If you are not so confident, take a quick algebra recap:\nRecap To permute a matrix $X$ is multiplying it by a permutation matrix $P$. Like:\n$$ X = \\begin{bmatrix} 1\u0026 2 \u0026 3 \u0026 4\\\\ 5\u0026 6 \u0026 7 \u0026 8\\\\ 9\u0026 10 \u0026 11 \u002612 \\end{bmatrix}, P = \\begin{bmatrix} 0\u0026 1 \u0026 0 \\\\ 0\u0026 0 \u0026 1 \\\\ 1\u0026 0 \u0026 0 \\end{bmatrix} $$\nThe order of $P$ here is $2,3,1$:\n$$PX = \\begin{bmatrix} 5\u0026 6 \u0026 7 \u0026 8\\\\ 9\u0026 10 \u0026 11 \u002612\\\\ 1\u0026 2 \u0026 3 \u0026 4\\\\ \\end{bmatrix}$$\nYou can calculate it by yourself if you don’t believe me.\nBy the way, left multiplication is the row permutation, and right multiplication is the column permutation.\nBy the way, the permutation matrix is an orthogonal matrix, which means $P^T = P^{-1}$.\nHere comes the truly interesting part. After I reported my findings to my dear advisor, she asked me, “What about the backward propagation?”\nBackward Propagation So I went back, taught myself some basic matrix calculus, and tried to calculate the total differential of Transformer. That’s crazy. I mean, I still can’t believe the total differential of a huge neural network can be written in one line. But it is. And it is quite simple.\nBefore we proceed, again, allow me to set up the training background. We denote the Transformer backbone as $f()$, excluding the embedding layer and the downstream head. In usual training settings, the output of backbone is:\n$$ Z = f(X) \\tag{1} $$\nwhere $X$ is the input. And in our permuted setting:\n$$ Z_{(P)} = P^\\top f(PX) \\tag{2} $$\nand we would like to denote all the variables in the above equation with a subscript $(P)$. Yeah, we need to permute them back. Although Transformer backbone works just fine on permuted data, the downstream head usually doesn’t.\n⚠️ To be continued… ","inspiration#Inspiration":"","invariance-equivariance#Invariance? Equivariance!":""},"title":"Permutation Equivariance"},"/research/hufu/":{"data":{"":"","our-solution#Our Solution":" ⚠️ Warning: This page is still under construction. Well this work is not yet published, so I must not share the very details of it. But here is a brief introduction of it.\nProblem Statement In AI safety, a lot of research stalls at decade old methods and tasks. I mean, most of the work focus on the same old MNIST, CIFAR-10 with VGG, ResNet, etc.\nTaking model watermarking for example, the BadNet backdoor is enhanced over and over again, some even dive into the decision boundary of the model. But now the paradigm is changed. We are dealing with larger models (I mean, ViT-base, GPT2 large, not ChatGPT large), and pretraining-and-finetuning is the new normal. The classifier is never the valuable part of the model. The true value lies in model backbone. We often use timm library to load the pretrained model, and the classifier is always discarded.\nAnd Transformer has blured the boundary between modalities. Transformer takes in tokens. Tokens can be anything. It can be image patches, it can be text, it can be audio, even video frames. The same old colorful trigger of badnet is no longer applicable.\nOur Solution We propose a modality-agnostic watermarking system for pre-trained Transformers via permutation equivariance. We call it Hufu.","problem-statement#Problem Statement":""},"title":"Hufu"}}