{"/aboutme/":{"data":{"":"","who-am-i#Who am I":"Well, although it is kind of silly to define oneself, I am an INFJ, a Master’s student who want to keep exploring AI as a researcher, and a gamer. But most importantly, to answer “who am I” , is up to you:\nFor potential employers, recruiters, and professors I am looking for a PhD program in the next three years. And I am also open to internships in AI research. If you are interested in my research, please contact me by email: doby-xu@outlook.com and I will be happy to send my latest CV.\nMy advisors tend to describe me as independent-minded, creative, suck at writing, but I prefer to describe myself as a person who keeps a tight schedule and is always curious about the world.\nCheck out my work on the permutation equivariance of Transformers and its applications. I mathematically formulated, extended and proved the permutation equivariance of Transformers in both row-wise and column-wise, in both forward and backward passes.\nMy Work Accepted by CVPR'24Permutation Equivariance of Transformer For my peers and fellow researchers I am a shy boy but I am always open to collaborations and discussions. No matter what you are interested in, like how to verify the permutation equivariance of Transformers, or how to beat Malenia, I am always happy to discuss with you.\nAnd also, my blogs would also be potentially helpful to you if you are trying to read my papers.\nFor junior students Although I am not one of the top graduate students in my cohort, I still got some experience to share. My experience may be highly specialized in how to apply for a Master’s program in China, and how to survive an academic journey to the west alone, so switch to my Chinese page if you are interested in that (the switch is on the bottom left corner)."},"title":"About Me"},"/blog/":{"data":{"":"Here are some of my thoughts, notes, experiences, and stories that I have to share."},"title":"Blog"},"/research/":{"data":{"":"Here are all my research works.\nAs a boy who spent the childhood playing games, it is hard for me to resist the temptation of computer science. And as a person who studied computer science, it is hard for me to resist the temptation of AI.\nAI is growing wildly. Just when we all thought more parameters and data would not break the bottleneck, large model showed the emergence of intelligence. I am more than honored to be a part of this era, and to witness all the astonishing breakthroughs, right in the field.\nPrivacy-Preserving Split Learning via Patch Shuffling over TransformersDixi Yao\rPermutation Equivariance of Transformers and Its ApplicationsHengyuan Xu\rHufuHengyuan Xu"},"title":"Research"},"/research/cvpr24/":{"data":{"":"","backward-propagation#Backward Propagation":"","inspiration#Inspiration":"","invariance-equivariance#Invariance? Equivariance!":"","pz-zp#PZ\u0026hellip; ZP?":"Actually, check out the 5 minutes video of our work on YouTube would be a good choice! All the gist of our work is there. And here, is mostly my personal view during the research process, and more mathematical details.\n❔ Transformer is somehow invariant to token permutation, but mathematically, how? How about intra-token permutation? What happens in the backward propagation? Here’s the answer! Inspiration We all know that Transformer is somehow invariant to token permutation. Like in the famous work, Intriguing Properties of Vision Transformers, as shown in the figure below:\nthey found that ViT is quite robust to patch shuffling. We all know some of the reasons, like:\nTokens are symmetric to self-attention. Transformer can only sense position information through positional encoding. etc. etc… But all of these explanations are quite intuitive, not rigorous. My advisor, Liyao Xiang, asked me,\n“Why? Why is Transformer invariant to token permutation? There must be a mathematical explanation behind it.”\nAnd that’s the beginning of our research, and my transformation.\nInvariance? Equivariance! At the beginning of this project, we were also quite curious about the permutation properties, especially from a mathematical perspective. We called it “invariance” at that time. However, if we looked into it a little bit deeper, we would soon find the real property was not called invariance, but equivariance.\n$$ f(Px) = Pf(x) \\tag{equivariance} $$\n$$ f(Px) = f(x) \\tag{invariance} $$\nI, found that, Transformer rigorously satisfies Eq. (equivariance). Actually, I was definitely not the first one to find this. Eq. (equivariance) is kind of a not-so-common common sense. So the Transformer is literally equivariant to token permutation, like this: Wait, how does it even work? Transformer is by all means not linear. How can it satisfy Eq. (equivariance)? Well, let’s dive into Transformer. The main parts of Transformer are:\nSelf-Attention linear projection softmax Feed-Forward linear projection element-wise activation Shortcut matrix addition Except for linear projection, all the other parts are element-wise operations (softmax is not… well, just go on reading, they are the same to permutation). Element-wise operations are permutation equivariant:\n$$ (P_1 A P_2) \\odot ( P_1 B P_2) = P_1 (A \\odot B) P_2 $$\nWhy? Because they are element-wise! On the left hand-side of the equation, $a_{ij}$ in $A$ and $b_{ij}$ in $B$ are permuted to the same position before being performed the operation. On the right hand-side, $a_{ij}$ and $b_{ij}$ are performed the operation first, then permuted to the same position. So the results are the same.\nHow about the linear projection? Well, it is linear!\n$$ P_1 (AW)= (P_1 A)W $$\nBefore we proceed, take a little quiz: are you familiar with the $P$ in Eq. (equivariance)? What does it mean? It is basic algebra, so basic that many people would forget about it. If you are not so confident, take a quick algebra recap:\nRecap To permute a matrix $X$ is multiplying it by a permutation matrix $P$. Like:\n$$ X = \\begin{bmatrix} 1\u0026 2 \u0026 3 \u0026 4\\\\ 5\u0026 6 \u0026 7 \u0026 8\\\\ 9\u0026 10 \u0026 11 \u002612 \\end{bmatrix}, P = \\begin{bmatrix} 0\u0026 1 \u0026 0 \\\\ 0\u0026 0 \u0026 1 \\\\ 1\u0026 0 \u0026 0 \\end{bmatrix} $$\nThe order of $P$ here is $2,3,1$:\n$$PX = \\begin{bmatrix} 5\u0026 6 \u0026 7 \u0026 8\\\\ 9\u0026 10 \u0026 11 \u002612\\\\ 1\u0026 2 \u0026 3 \u0026 4 \\end{bmatrix}$$\nYou can calculate it by yourself if you don’t believe me.\nBy the way, left multiplication is the row permutation, and right multiplication is the column permutation.\nBy the way, the permutation matrix is an orthogonal matrix, which means $P^\\top = P^{-1}$.\nHere comes the truly interesting part. After I reported my findings to my dear advisor, she asked me,\n“What about the backward propagation? What happens in the backward?”\nBackward Propagation So I went back, taught myself some basic matrix calculus, and tried to calculate the total differential of Transformer. That’s crazy. I mean, I still can’t believe the total differential of a huge neural network can be written in one line. But it is. And it is quite simple.\nAt first, it does not make sense – the learning process is totally neither invariant nor equivariant.\nFormulation of Transformer To explain this, we need to formulate Transformer and its training process. Normally in math we denote a vector as a column vector. But to make it less confusing when we implement it with PyTorch, here we denote a token as a row vector. The Transformer looks like this:\nOK, so the input of Transformer is a sequence of tokens, denoted as $Z$, like in ViT-Base, $Z\\in \\mathbb{R}^{197\\times 768}$. When $Z$ enters the Transformer, it meets $W_Q, W_K, W_V$ first, and Let’s say:\n$$ Q = ZW_Q^\\top$$ $$ K = ZW_K^\\top$$ $$ V = ZW_V^\\top$$\nThen, the self-attention is calculated as:\n$$ S = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) $$ $$ A = SV $$\nWe neglect the residual connection and the attention projection for simplicity. The MLP feed-forward is calculated as:\n$$ A_1 = A W_1^\\top $$ $$ H = \\text{ReLU}(A_1) $$ $$ A_2 = H W_2^\\top $$\nTotal Differential After layers of those, we get the output of Transformer, which, let’s denote as $A_3$. Actually we only need to consider one layer, and the induction will do the rest, so let’s say $A_3 = A_2$. And…let’s just make the downstream head a linear: $O = A_3 W^\\top$. So the gradient is:\n$$ \\begin{aligned} \\text{d}l \u0026= \\text{tr}\\left(\\frac{\\partial l}{\\partial O}^\\top \\text{d}O\\right)\\\\ \u0026= \\text{tr}\\left(\\frac{\\partial l}{\\partial O}^\\top( \\text{d}A_3)W^\\top \\right) + \\text{tr}\\left(\\frac{\\partial l}{\\partial O}^\\top A_3 \\text{d}W^\\top\\right)\\\\ \\end{aligned} $$\nwhere $l$ is the loss function.\nAllow me to direct you to the text book, Matrix Calculus: Derivation and Simple Application HU Pili, if you are unfamiliar with the derivation above. I assure you that after reading it, you will find the derivation of Transformer is quite simple.\nThat indicates: $$ \\frac{\\partial l}{\\partial A_3} = \\frac{\\partial l}{\\partial O}W $$ and: $$ \\frac{\\partial l}{\\partial W} = \\frac{\\partial l}{\\partial O}^\\top A_3 $$ So in the forward propagation, we know the value of $A_3$ and $W$. Once we know $\\frac{\\partial l}{\\partial O}$, we can calculate the gradient of $W$ and $A_3$. As for the value of $\\frac{\\partial l}{\\partial O}$, well, forget about it. PyTorch knows it and that’s enough. To us, it is just a known value.\nPermute It! Emmm…. Now you see, if we permute the input $Z$ to $PZ$, the output would be $A_3 = P A_2$. And the gradient would be:\n$$ \\begin{align*} \\text{d}l_{(P)} \u0026= \\text{tr}\\left(\\frac{\\partial l_{(P)}}{\\partial O_{(P)}}^\\top( \\text{d}PA_2)W^\\top \\right) + \\text{tr}\\left(\\frac{\\partial l_{(P)}}{\\partial O_{(P)}}^\\top PA_2 \\text{d}W^\\top\\right)\\\\ \\end{align*} $$ where we denote all the variables in permuted setting with a subscript $(P)$. Like $$ A_{2(P) }= P A_2$$\nThis…just makes no sense.\nFor nights and nights I stared at the equations, it suddenly hit me, that if we permute $PA_2$ back, things would be different:\n$$ A_{3(P)} = P^\\top A_{2(P)} = P^\\top \\cdot P A_2 = A_2 = A_3 $$\nNow, since $A_3 = A_{3(P)}$, all the following things, loss, and gradient, would be the same. $$ l_{(P)} = l, O_{(P)} = O $$ $$ \\frac{\\partial l_{(P)}}{\\partial O_{(P)}} = \\frac{\\partial l}{\\partial O} $$ $$ \\frac{\\partial l_{(P)}}{\\partial A_{3(P)}} = \\frac{\\partial l}{\\partial A_3} $$\nOK, pause here. Before we proceed, I sincerely recommend you to take a look at the relationship between $A_3, A_2, A_{3(P)}, A_{2(P)}$. What we are trying to do here is to find a bridge between the original setting and the permuted setting, especially for the gradients of weights $W_Q, W_K, W_V, W_1, W_2$. To get their, we must pass the entrance – the relationship between $\\frac{\\partial l_{(P)}}{\\partial A_{2(P)}}$ and $\\frac{\\partial l}{\\partial A_2}$.\nNow let’s focus on the relationship between $\\frac{\\partial l_{(P)}}{\\partial A_{2(P)}}$ and $\\frac{\\partial l}{\\partial A_2}$. Since $A_{3(P)} = P^\\top A_{2(P)}$,\n$$ \\begin{align*} \\text{d}l_{(P)} \u0026= \\text{tr}\\left(\\frac{\\partial l_{(P)}}{\\partial A_{3(P)}}^\\top P^\\top \\text{d}A_{2(P)}\\right) \\\\ \u0026= \\text{tr}\\left((P\\frac{\\partial l_{(P)}}{\\partial A_{3(P)}})^\\top \\text{d}A_{2(P)}\\right) \\\\ \\end{align*} $$ and thus: $$ \\frac{\\partial l_{(P)}}{\\partial A_{2(P)}} = P\\frac{\\partial l_{(P)}}{\\partial A_{3(P)}} = P\\frac{\\partial l}{\\partial A_3} = P\\frac{\\partial l}{\\partial A_2} $$\nYou see what’s happening here? The gradient of $A_2$ in the permuted setting, equals to the gradient of $A_2$ in the original setting, permuted!\nWait, we want to know the gradient of weights, right? Eq. () tells us that: $$ \\frac{\\partial l}{\\partial W_2} = \\frac{\\partial l}{\\partial A_2}^\\top H $$ so $$\\begin{align*} \\frac{\\partial l_{(P)}}{\\partial W_{2(P)}} \u0026= \\frac{\\partial l_{(P)}}{\\partial A_{2(P)}}^\\top H_{(P)}\\\\ \u0026= \\frac{\\partial l}{\\partial A_2}^\\top P^\\top PH\\\\ \u0026= \\frac{\\partial l}{\\partial A_2}^\\top H\\\\ \u0026= \\frac{\\partial l}{\\partial W_2} \\end{align*}$$\nOK, we can pop the champagne now. ℹ️ Theory 1: The backward gradient of weights in the permuted setting, is the same as the original setting! Or may be later, otherwise we would be drunk before we finish the paper. There are a lot of champagne moments later. For me, there were even more. After I found this, I immediately turned on my computer, and fine-tuned a ViT model on CIFAR-10, permuted of course. And it worked! All the properties above were verified. Ah, nothing is more satisfying than seeing the theory works in practice.\nPZ… ZP? Even though I was quite excited about the result, and the scientifically satisfying moment is surely a transformative experience, this is not enough for a paper.\nThen one night I was in the bed, Q and K and V and P were all floating in my mind. Suddenly, floating P met floating Z, not like “PZ”, but like “ZP”.\nHoly, how could I never think of this? Left multiplication is the row permutation, and right multiplication is the column permutation.\nNah, that makes no sense: $$ ZPW \\neq ZW$$\nLater, another P floated by: $$ ZP \\cdot P^\\top W = ZW$$\n⚠️ To be continued…\nMan! this is a long story.\nAnd I would tag all the equations after I finish this blog."},"title":"Permutation Equivariance"},"/research/hufu/":{"data":{"":"","our-solution#Our Solution":" ⚠️ Warning: This page is still under construction. Well this work is not yet published, so I must not share the very details of it. But here is a brief introduction of it.\nProblem Statement In AI safety, a lot of research stalls at decade old methods and tasks. I mean, most of the work focus on the same old MNIST, CIFAR-10 with VGG, ResNet, etc.\nTaking model watermarking for example, the BadNet backdoor is enhanced over and over again, some even dive into the decision boundary of the model. But now the paradigm is changed. We are dealing with larger models (I mean, ViT-base, GPT2 large, not ChatGPT large), and pretraining-and-finetuning is the new normal. The classifier is never the valuable part of the model. The true value lies in model backbone. We often use timm library to load the pretrained model, and the classifier is always discarded.\nAnd Transformer has blured the boundary between modalities. Transformer takes in tokens. Tokens can be anything. It can be image patches, it can be text, it can be audio, even video frames. The same old colorful trigger of BadNet is no longer applicable.\nOur Solution We propose a modality-agnostic watermarking system for pre-trained Transformers via permutation equivariance. We call it Hufu. Check out our paper for more details.","problem-statement#Problem Statement":""},"title":"Hufu"}}