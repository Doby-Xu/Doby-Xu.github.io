{"/zh/aboutme/":{"data":{"":"","我是谁#我是谁":"嗯…给自己下定义貌似有点愚蠢，但是吧，我可能是个INFJ，是一个想要继续以研究者的身份探索AI的硕士生，也是个游戏玩家。但是，最重要的是，回答“我是谁”这个问题，取决于你：\n对于潜在的招聘者和导师 I am looking for a PhD program in the next three years. 并且我也期待着在AI业界的实习。如果你对我的研究感兴趣，请通过邮件联系我： doby-xu@outlook.com。我会发送我的最新简历。\n我的导师们倾向于描述我为独立思考、有创造力、并且写的文章不忍卒读，但我更愿意描述自己为一个保持严格时间计划并且对世界充满好奇的人。\n您可以来看看我的关于Transformer的重排等变性的工作。在这篇工作中，我详尽阐释并拓展了Transformer的重排等变性，用严谨的数学和详细的实验证明了它。\nMy Work Accepted by CVPR'24Permutation Equivariance of Transformer 对于我的同行和研究者 我可能比较慢热，但是我总是乐于合作和讨论。无论你对什么感兴趣，比如如何复现Transformer的重排等变性，或者如何无伤拉塔恩，欢迎与我讨论。\n以及，我的博客也可能对你有所帮助，如果你想阅读我的论文的话。\n对于学弟学妹 我当然算不上大佬，刚上大学时也只是小镇做题家，但也正因如此，我有一些亲民的经验可以分享。我在大三时、准备保研时、收集了各路大佬的夏令营经验，发现这种经验多多益善，在此我也想分享给大家；我在参加CVPR前，并没有搜集到很多参会经验，我希望我的经验可以为大家提供帮助。"},"title":"关于我"},"/zh/blog/":{"data":{"":"这里是我的blog\n或许是一些小项目，一些想法，一些笔记，一些经验，或者是不得不分享的故事"},"title":"Blog"},"/zh/blog/experience/":{"data":{"":"我来自河南一个十八线小城，那里的高中将衡水模式奉为圭臬，我是一个名副其实的小镇做题家。也正因如此，我的经验或许对近些年的学弟学妹更有普适的帮助。"},"title":"经验谈"},"/zh/blog/experience/master_apply/":{"data":{"":"","tldr#TL;DR":"","交大#交大":"","备考经验#备考经验":"TL;DR ℹ️ TL;DR: 总体上的成功经验是：导师更喜欢一进组就能干活的同学，英语强，有科研经验（写过论文做过实验的）加大分。有些资本是要靠前几年的积累，有些知识则要短时间突击。 先说笔者保研时的资本：\n上海交通大学电院末流专业，成绩5%，六级618（好像是），有比较丰富的科研经历，夏令营时有一篇三作ICDM，一篇NeurIPS一作在投。\n夏令营选择 夏令营海了三个：\n复旦CS，接受学硕offer 交大CS，机考崩了，拒绝了专硕offer 自动化所，过了面试，进入双选前接受复旦offer，自行退出。 整体上三家的考核风格是：\n对机考/代码能力的重视程度：交大 \u003e 复旦 \u003e 自动化所 对面试/导师关系的重视程度：交大 \u003c 复旦 \u003c 自动化所 交大 总结： 机考巨难，没搞过信息竞赛建议LeetCode迅速走起（大三的话现在开始准备好像有亿点晚了）\n机考和面试的比重1:1，所以如果没有太多科研经验，没有信心在面试的时候俘获导师的芳心，机考要好好准备。笔者那年机考有三道题，个人感觉均是LeetCode hard难度，字里行间写满了“你不配”。\n考核方式： 交大的考核中，是200分，机考面试各100，考核结束后按排名给offer，排名靠前的拒绝了offer，会顺次轮给下面的同学。录取率还挺高的。但是这也导致导师操作空间很小（就算导师拉关系给你把面试搞满分，机考崩了也是崩了）\n机考： 笔者前一届机考两题，一题是手搓TSP，一题是手搓计算器。难度笔者便不评价了。笔者这一届三道题，一是最小生成树，二忘记了，三是手搓Round Robin调度算法。\n面试： 面试的话，其实各家都差不多，大致分为几个方面问你：\n- 基础知识，高数、概率和数据结构。会突然间有一段英语问答。\r- 交大好像没有问我基础知识，只记得突然来了句 “Who's your advisor?”\r- 科研经历，问你做过什么，老师感兴趣的话会追着问。不要不懂装懂，老师很可能比你懂得多。\r- 由于我一直在交大做科研，确实没有难为我，问了问我NeurIPS的rebuttal在什么时候，问了问我“传感器原理”这门课讲了什么。\r大概就是这样。笔者科研经历比较丰富，所以面试没有太为难，后面就是闲聊了。\n复旦 总结： 机考比较简单，LeetCode简单和中等水平。机考不直接计分，但是会作为面试依据。\n考核方式： 除了机考、面试外，另外还有一轮英语考核，大概就是纯英环境下和你聊两句。\n由于接了复旦的offer，这里不过多透漏细节了。总体来说，复旦比较注重面试。\n自动化所 总结： 这个比较特殊，没有机考。但是要注意的是，中科院自动化所是科研院所，不是高校。区别事实上是很大的。\n导师鱼龙混杂，不过对非华五的学生也比较友好。 如果你之后想工作的话，出来之后，有些只认985的岗位可能会卡简历 据传，进组之后做得更多的是横向，很少会有做自己项目的机会。如果目标是攒成果出国的话，这里可能不是最好的选择。 考核方式： 公认的是，最高的门槛在于简历，只要简历过了，进夏令营了，80%的都能有书读。只一轮面试，过了就会进入和导师双选的阶段。\n面试： 和交大面试差不多，问题如下：\n- 费马定理是什么？不知道的话，罗尔定理是什么？\r- What's your hobby?\r- 你的之前这个研究是什么。\r- 你说你想出国，你为什么要来自动化所，你做好功课了吗？给了你offer会鸽我们吗？\r没有参加完整个流程，所以这家的经验不多。\n备考经验 前置准备 私以为最重要的事情不是备考，而是大一大二时的资本积累。最重要的是科研经历，其次是六级分数和GPA。\n当然，看到此处的你可能已经大三了，那么能做的便是开始备考，或是紧急刷一刷科研经历什么的。\n基础知识 高数、概统和数据结构是比较重要的两个学科。基本的泰勒公式、中值定理掌握明白。应该问题不大。至于概统，可能会当场给你出一道题。复习复习高数概统对之后的研究也大有裨益，所以尽情地复习吧！\n数据结构的话，据传堆栈区别，以及红黑树是个热门题目。\n此外，大部分人都和AI沾点边，于是乎SVM也成为了一个热门题目。\n机考 学长给笔者的经验是，从大三寒假就开始刷LeetCode，一天一道中等一道困难，刷完之后看看题解，背一背经典的算法。对于没有信息竞赛经历的小白如我，机考准备尤为重要。\n简历 简历则要尤为突出自己的强项，尤其是科研经历。哪怕没有科研经历，也可以写一些课程项目什么的。","复旦#复旦":"","夏令营选择#夏令营选择":"","自动化所#自动化所":""},"title":"夏令营经验"},"/zh/blog/paper/":{"data":{"":"总是要读论文的，顺便把笔记po到这里"},"title":"Cool Papers"},"/zh/blog/paper/neurallineage/":{"data":{"notes#Notes":"","分类任务cifar10mnist#分类任务（Cifar10、MNIST）":"Notes摘要 提出了两种方法：\n一种 learning-free 方法，将“fine tune过程的近似”集成进基于相似度的谱系检测 学习方法，表征好后丢给Transformer 引言 背景 Pretrain-and-finetune范式已经取代了Train from scratch，Deep learning models are no longer isolated 多种Model Zoo/Repo的应用织出了一张庞大而多样的模型网络 任务定义 从一堆condidate parent models ${f_p^{(m)}}^M_{m=1}$中找出child model $f_c$ 是从哪个$f_p^{(m)}$微调出来的\n可能的应用 model reuse 了解知识的继承，揭示泛化性、鲁棒性、偏见和公平性 IP protection model regulation 追迹、问责、监管 相关工作 IP保护 和IP保护有一些显著的区别：IP保护是2分类任务，即一个模型是不是侵犯了某个模型的IP；谱系检测时多分类，要识别哪个是母亲 目前IP检测解决方案中多使用external media，需要extra training，optimization or search。谱系检测不希望对模型有额外编码、修改或嵌入水印 IP保护语境中，“将finetune视为一种攻击方法”，往往采用小学习率和小轮数，只在同一个数据集上重新训 (?) 神经网络表示相似性 神经网络线性化 实验章节分类任务（Cifar10、MNIST） Setup 模型结构 采用两种：\n三层全连接（FC）加一个ReLU ResNet18 选用这两种网络的主要原因：\n为了得到足够多的模型，构建代系数据集。因为要微调很多很多模型，所以模型小一点方便构建 全连接网络更符合他们learning-free方法的设定，可以在相对理想的条件下测试他 注意：用全连接层训练的网络，可以泛化并推断ViT的谱系\n数据集\nParent (train from scratch)： FC网络：20个来自MNIST 12个来自Cifar100 ResNet18：7个来自timm (ImageNet Pretrained) Child FMNIST, EMNIST-Letters, Cifar10, Pet, DTD 区别不同模型的方式（模型之间的区别）\n随机种子 超参数 Q：为什么调超参数？网络的灵魂不是数据集和训练策略吗？\nA：用调超参数来构造 如果认为不同数据集/不同训练策略的微调才能区别子模型，那么任务将会过于简单，各种数据集上都是100%不太好看，也不太能和baseline区分\n子母模型训练细则\n对于20个MNIST上的FC网络，采用了3种batch size，3种学习率和4个随机数种子，训练了50轮，得到了334=36个模型，选取了Acc top 20. 这是20个FC母网络\n在这20个母网络上，以3种学习率，2种batch size，4个随机种子，两个数据集F/E-MNIST上，训练36轮，每种数据集上得到480个子模型，选取acc高于80%的模型，每个数据集得到了228个子模型\n其他\ntrain : validation : test = 7 : 1 : 2\n5 fold 取平均\n主要实验观察 learning-based 方法显著好于 learning-free，平均高36个百分点 learning free中，总体上大家表现参差不齐 few-shot时谱系检测比完整微调要简单 提高学习率和微调epoch数会提高learning free方法的检测难度，但对learning based方法影响不大 Detection \u0026 Segmentation 采用了DETR_ResNet50模型，Segmentation的谱系检测率普遍偏低\nViT-Base HyBird Model 采用了FC网络上训练的网络，去检测混合的ViT模型\n母模型：We concatenate the sub-network of the hybrid model before the i-th transformer layer with the i-th transformer layer of each of the nine parent models\n子模型： the sub-network of the hybrid model up to the i-th transformer layer is regarded as the child model\n只采用了该层的中层特征去检测","实验章节#实验章节":"","引言#引言":"","摘要#摘要":"","相关工作#相关工作":""},"title":"Neural Lineage"},"/zh/research/":{"data":{"":"这里是我的研究工作。\n一个从小打游戏长大的孩子，很难抗拒计算机科学的诱惑。一个学习计算机科学的人，很难抗拒人工智能的诱惑。\n人工智能正在疯狂地发展。20年，正当我们都以为堆参数量和数据量难以突破瓶颈之时，大模型横空出世，涌现着惊人的智能。很荣幸可以身处这个时代，在一线见证所有惊人的突破\nPrivacy-Preserving Split Learning via Patch Shuffling over TransformersDixi Yao\rPermutation Equivariance of Transformers and Its ApplicationsHengyuan Xu\rHufuHengyuan Xu"},"title":"研究工作"}}